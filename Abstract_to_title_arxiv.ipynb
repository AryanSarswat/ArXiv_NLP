{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1ea7c38a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-05T14:02:03.142888Z",
     "start_time": "2024-04-05T14:02:03.139166Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64671f92",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-05T13:47:18.115211Z",
     "start_time": "2024-04-05T13:47:18.059029Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32e5a722",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-05T13:47:27.716681Z",
     "start_time": "2024-04-05T13:47:27.601110Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Apr  5 09:47:27 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 552.12                 Driver Version: 552.12         CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                     TCC/WDDM  | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce GTX 1660 Ti   WDDM  |   00000000:01:00.0  On |                  N/A |\n",
      "| N/A   56C    P8             10W /   80W |     724MiB /   6144MiB |     12%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      4236    C+G   ...aming\\Telegram Desktop\\Telegram.exe      N/A      |\n",
      "|    0   N/A  N/A      6764    C+G   ...5n1h2txyewy\\ShellExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A      6932    C+G   ...\\cef\\cef.win7x64\\steamwebhelper.exe      N/A      |\n",
      "|    0   N/A  N/A      8300    C+G   ...on\\123.0.2420.65\\msedgewebview2.exe      N/A      |\n",
      "|    0   N/A  N/A      8608    C+G   C:\\Windows\\explorer.exe                     N/A      |\n",
      "|    0   N/A  N/A     10100    C+G   ...1.0_x64__cv1g1gvanyjgm\\WhatsApp.exe      N/A      |\n",
      "|    0   N/A  N/A     10160    C+G   ...nt.CBS_cw5n1h2txyewy\\SearchHost.exe      N/A      |\n",
      "|    0   N/A  N/A     10184    C+G   ...2txyewy\\StartMenuExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A     10636    C+G   ...__8wekyb3d8bbwe\\WindowsTerminal.exe      N/A      |\n",
      "|    0   N/A  N/A     12828    C+G   ...t.LockApp_cw5n1h2txyewy\\LockApp.exe      N/A      |\n",
      "|    0   N/A  N/A     14060    C+G   ...CBS_cw5n1h2txyewy\\TextInputHost.exe      N/A      |\n",
      "|    0   N/A  N/A     14684    C+G   ...B\\system_tray\\lghub_system_tray.exe      N/A      |\n",
      "|    0   N/A  N/A     15096    C+G   ... Synapse 3 Host\\Razer Synapse 3.exe      N/A      |\n",
      "|    0   N/A  N/A     15656    C+G   ...a\\Local\\slack\\app-4.37.98\\slack.exe      N/A      |\n",
      "|    0   N/A  N/A     15728    C+G   ...ces\\Razer Central\\Razer Central.exe      N/A      |\n",
      "|    0   N/A  N/A     18792    C+G   ...siveControlPanel\\SystemSettings.exe      N/A      |\n",
      "|    0   N/A  N/A     23516    C+G   ...crosoft\\Edge\\Application\\msedge.exe      N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
    "  print('and then re-execute this cell.')\n",
    "else:\n",
    "  print(gpu_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "93174e4f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-05T13:55:44.183978Z",
     "start_time": "2024-04-05T13:55:44.180250Z"
    }
   },
   "outputs": [],
   "source": [
    "def print_list(l, K=None):\n",
    "\tfor i, e in enumerate(l):\n",
    "\t\tif i == K:\n",
    "\t\t\tbreak\n",
    "\t\tprint(e)\n",
    "\tprint()\n",
    "\n",
    "def load_from_pickle(pickle_file):\n",
    "\twith open(pickle_file, \"rb\") as pickle_in:\n",
    "\t\treturn pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea066862",
   "metadata": {},
   "source": [
    "# Data Loading and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9402b928",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-05T13:53:28.899229Z",
     "start_time": "2024-04-05T13:53:28.740016Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>categories</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2007</th>\n",
       "      <td>0704.2008</td>\n",
       "      <td>Observations of Manifestations of Skeletal Str...</td>\n",
       "      <td>The analysis of databases of photographic im...</td>\n",
       "      <td>astro-ph</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7988</th>\n",
       "      <td>0705.3986</td>\n",
       "      <td>Variable Electron-Phonon Coupling in Isolated ...</td>\n",
       "      <td>We report the existence of broad and weakly ...</td>\n",
       "      <td>cond-mat.mtrl-sci</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9269</th>\n",
       "      <td>0706.0583</td>\n",
       "      <td>Determination of InN-GaN heterostructure band ...</td>\n",
       "      <td>Band discontinuities at the InN-GaN heteroin...</td>\n",
       "      <td>cond-mat.mtrl-sci cond-mat.other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>0704.0103</td>\n",
       "      <td>Generalized regularly discontinuous solutions ...</td>\n",
       "      <td>The physical consistency of the match of pie...</td>\n",
       "      <td>gr-qc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1383</th>\n",
       "      <td>0704.1384</td>\n",
       "      <td>Generalizing circles over algebraic extensions</td>\n",
       "      <td>This paper deals with a family of spatial ra...</td>\n",
       "      <td>math.AG</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                              title  \\\n",
       "2007  0704.2008  Observations of Manifestations of Skeletal Str...   \n",
       "7988  0705.3986  Variable Electron-Phonon Coupling in Isolated ...   \n",
       "9269  0706.0583  Determination of InN-GaN heterostructure band ...   \n",
       "102   0704.0103  Generalized regularly discontinuous solutions ...   \n",
       "1383  0704.1384     Generalizing circles over algebraic extensions   \n",
       "\n",
       "                                               abstract  \\\n",
       "2007    The analysis of databases of photographic im...   \n",
       "7988    We report the existence of broad and weakly ...   \n",
       "9269    Band discontinuities at the InN-GaN heteroin...   \n",
       "102     The physical consistency of the match of pie...   \n",
       "1383    This paper deals with a family of spatial ra...   \n",
       "\n",
       "                            categories  \n",
       "2007                          astro-ph  \n",
       "7988                 cond-mat.mtrl-sci  \n",
       "9269  cond-mat.mtrl-sci cond-mat.other  \n",
       "102                              gr-qc  \n",
       "1383                           math.AG  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = ['id', 'title', 'abstract', 'categories']\n",
    "data = []\n",
    "file_name = './arxiv-metadata-oai-snapshot.json'\n",
    "\n",
    "with open(file_name, encoding='latin-1') as f:\n",
    "    count = 0\n",
    "    for line in f:\n",
    "        doc = json.loads(line)\n",
    "        lst = [doc['id'], doc['title'], doc['abstract'], doc['categories']]\n",
    "        data.append(lst)\n",
    "        count += 1\n",
    "        if count >= 10000:\n",
    "            break\n",
    "        \n",
    "df = pd.DataFrame(data=data, columns=cols).sample(n=100, random_state=68)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "14cefe10",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-05T14:04:05.909496Z",
     "start_time": "2024-04-05T14:04:04.648061Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words in the vocabulary = 36292\n"
     ]
    }
   ],
   "source": [
    "pad_word = \"<pad>\"\n",
    "bos_word = \"<s>\"\n",
    "eos_word = \"</s>\"\n",
    "unk_word = \"<unk>\"\n",
    "pad_id = 0\n",
    "bos_id = 1\n",
    "eos_id = 2\n",
    "unk_id = 3\n",
    "\n",
    "def normalize_sentence(s):\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    s = re.sub(r\"\\s+\", r\" \", s).strip()\n",
    "    return s\n",
    "\n",
    "class Vocabulary:\n",
    "    def __init__(self):\n",
    "        self.word_to_id = {pad_word: pad_id, bos_word: bos_id, eos_word:eos_id, unk_word: unk_id}\n",
    "        self.word_count = {}\n",
    "        self.id_to_word = {pad_id: pad_word, bos_id: bos_word, eos_id: eos_word, unk_id: unk_word}\n",
    "        self.num_words = 4\n",
    "\n",
    "    def get_ids_from_sentence(self, sentence):\n",
    "        sentence = normalize_sentence(sentence)\n",
    "        sent_ids = [bos_id] + [self.word_to_id[word] if word in self.word_to_id \\\n",
    "                               else unk_id for word in sentence.split()] + \\\n",
    "                               [eos_id]\n",
    "        return sent_ids\n",
    "\n",
    "    def tokenized_sentence(self, sentence):\n",
    "        sent_ids = self.get_ids_from_sentence(sentence)\n",
    "        return [self.id_to_word[word_id] for word_id in sent_ids]\n",
    "\n",
    "    def decode_sentence_from_ids(self, sent_ids):\n",
    "        words = list()\n",
    "        for i, word_id in enumerate(sent_ids):\n",
    "            if word_id in [bos_id, eos_id, pad_id]:\n",
    "                # Skip these words\n",
    "                continue\n",
    "            else:\n",
    "                words.append(self.id_to_word[word_id])\n",
    "        return ' '.join(words)\n",
    "\n",
    "    def add_words_from_sentence(self, sentence):\n",
    "        sentence = normalize_sentence(sentence)\n",
    "        for word in sentence.split():\n",
    "            if word not in self.word_to_id:\n",
    "                # add this word to the vocabulary\n",
    "                self.word_to_id[word] = self.num_words\n",
    "                self.id_to_word[self.num_words] = word\n",
    "                self.word_count[word] = 1\n",
    "                self.num_words += 1\n",
    "            else:\n",
    "                # update the word count\n",
    "                self.word_count[word] += 1\n",
    "\n",
    "abstracts_only = []\n",
    "titles_only = []\n",
    "                \n",
    "vocab = Vocabulary()\n",
    "for ids, title, abstract, categories in data:\n",
    "    abstracts_only.append(abstract)\n",
    "    titles_only.append(title)\n",
    "    vocab.add_words_from_sentence(title)\n",
    "    vocab.add_words_from_sentence(abstract)\n",
    "print(f\"Total words in the vocabulary = {vocab.num_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "85fa5355",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-05T13:55:46.730680Z",
     "start_time": "2024-04-05T13:55:46.716749Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('the', 92050)\n",
      "('of', 64248)\n",
      "('.', 59238)\n",
      "('and', 32690)\n",
      "('a', 30864)\n",
      "('in', 26941)\n",
      "('to', 23517)\n",
      "('is', 18678)\n",
      "('for', 14358)\n",
      "('with', 12928)\n",
      "('that', 12731)\n",
      "('We', 11592)\n",
      "('The', 10923)\n",
      "('are', 9531)\n",
      "('on', 9162)\n",
      "('by', 8526)\n",
      "('we', 7280)\n",
      "('as', 6171)\n",
      "('an', 6127)\n",
      "('from', 5913)\n",
      "('be', 5863)\n",
      "('at', 5715)\n",
      "('this', 5527)\n",
      "('which', 5012)\n",
      "('In', 3955)\n",
      "('model', 3682)\n",
      "('can', 3608)\n",
      "('two', 3510)\n",
      "('A', 3418)\n",
      "('field', 3107)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_list(sorted(vocab.word_count.items(), key=lambda item: item[1], reverse=True), 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2f756730",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-05T13:57:05.191697Z",
     "start_time": "2024-04-05T13:57:05.185555Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  A fully differential calculation in perturbative quantum chromodynamics is\n",
      "presented for the production of massive photon pairs at hadron colliders. All\n",
      "next-to-leading order perturbative contributions from quark-antiquark,\n",
      "gluon-(anti)quark, and gluon-gluon subprocesses are included, as well as\n",
      "all-orders resummation of initial-state gluon radiation valid at\n",
      "next-to-next-to-leading logarithmic accuracy. The region of phase space is\n",
      "specified in which the calculation is most reliable. Good agreement is\n",
      "demonstrated with data from the Fermilab Tevatron, and predictions are made for\n",
      "more detailed tests with CDF and DO data. Predictions are shown for\n",
      "distributions of diphoton pairs produced at the energy of the Large Hadron\n",
      "Collider (LHC). Distributions of the diphoton pairs from the decay of a Higgs\n",
      "boson are contrasted with those produced from QCD processes at the LHC, showing\n",
      "that enhanced sensitivity to the signal can be obtained with judicious\n",
      "selection of events.\n",
      "\n",
      "['<s>', 'A', 'fully', 'differential', 'calculation', 'in', 'perturbative', 'quantum', 'chromodynamics', 'is', 'presented', 'for', 'the', 'production', 'of', 'massive', 'photon', 'pairs', 'at', 'hadron', 'colliders', '.', 'All', 'next', 'to', 'leading', 'order', 'perturbative', 'contributions', 'from', 'quark', 'antiquark', 'gluon', 'anti', 'quark', 'and', 'gluon', 'gluon', 'subprocesses', 'are', 'included', 'as', 'well', 'as', 'all', 'orders', 'resummation', 'of', 'initial', 'state', 'gluon', 'radiation', 'valid', 'at', 'next', 'to', 'next', 'to', 'leading', 'logarithmic', 'accuracy', '.', 'The', 'region', 'of', 'phase', 'space', 'is', 'specified', 'in', 'which', 'the', 'calculation', 'is', 'most', 'reliable', '.', 'Good', 'agreement', 'is', 'demonstrated', 'with', 'data', 'from', 'the', 'Fermilab', 'Tevatron', 'and', 'predictions', 'are', 'made', 'for', 'more', 'detailed', 'tests', 'with', 'CDF', 'and', 'DO', 'data', '.', 'Predictions', 'are', 'shown', 'for', 'distributions', 'of', 'diphoton', 'pairs', 'produced', 'at', 'the', 'energy', 'of', 'the', 'Large', 'Hadron', 'Collider', 'LHC', '.', 'Distributions', 'of', 'the', 'diphoton', 'pairs', 'from', 'the', 'decay', 'of', 'a', 'Higgs', 'boson', 'are', 'contrasted', 'with', 'those', 'produced', 'from', 'QCD', 'processes', 'at', 'the', 'LHC', 'showing', 'that', 'enhanced', 'sensitivity', 'to', 'the', 'signal', 'can', 'be', 'obtained', 'with', 'judicious', 'selection', 'of', 'events', '.', '</s>']\n",
      "[1, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 8, 5, 28, 29, 30, 11, 31, 32, 33, 34, 35, 36, 37, 38, 21, 39, 40, 41, 42, 43, 44, 41, 13, 43, 43, 45, 46, 47, 48, 49, 48, 50, 51, 52, 5, 53, 54, 43, 55, 56, 11, 35, 36, 35, 36, 37, 57, 58, 33, 59, 60, 5, 61, 62, 24, 63, 20, 64, 27, 19, 24, 65, 66, 33, 67, 68, 24, 69, 70, 71, 40, 27, 72, 12, 13, 73, 46, 74, 26, 75, 76, 77, 70, 78, 13, 79, 71, 33, 80, 46, 81, 26, 82, 5, 7, 30, 83, 11, 27, 84, 5, 27, 85, 86, 87, 14, 33, 88, 5, 27, 7, 30, 40, 27, 89, 5, 90, 91, 92, 46, 93, 70, 94, 83, 40, 95, 96, 11, 27, 14, 97, 98, 99, 100, 36, 27, 101, 102, 103, 104, 70, 105, 106, 5, 107, 33, 2]\n",
      "A fully differential calculation in perturbative quantum chromodynamics is presented for the production of massive photon pairs at hadron colliders . All next to leading order perturbative contributions from quark antiquark gluon anti quark and gluon gluon subprocesses are included as well as all orders resummation of initial state gluon radiation valid at next to next to leading logarithmic accuracy . The region of phase space is specified in which the calculation is most reliable . Good agreement is demonstrated with data from the Fermilab Tevatron and predictions are made for more detailed tests with CDF and DO data . Predictions are shown for distributions of diphoton pairs produced at the energy of the Large Hadron Collider LHC . Distributions of the diphoton pairs from the decay of a Higgs boson are contrasted with those produced from QCD processes at the LHC showing that enhanced sensitivity to the signal can be obtained with judicious selection of events .\n",
      "\n",
      "  We describe a new algorithm, the $(k,\\ell)$-pebble game with colors, and use\n",
      "it obtain a characterization of the family of $(k,\\ell)$-sparse graphs and\n",
      "algorithmic solutions to a family of problems concerning tree decompositions of\n",
      "graphs. Special instances of sparse graphs appear in rigidity theory and have\n",
      "received increased attention in recent years. In particular, our colored\n",
      "pebbles generalize and strengthen the previous results of Lee and Streinu and\n",
      "give a new proof of the Tutte-Nash-Williams characterization of arboricity. We\n",
      "also present a new decomposition that certifies sparsity based on the\n",
      "$(k,\\ell)$-pebble game with colors. Our work also exposes connections between\n",
      "pebble game algorithms and previous sparse graph algorithms by Gabow, Gabow and\n",
      "Westermann and Hendrickson.\n",
      "\n",
      "['<s>', 'We', 'describe', 'a', 'new', 'algorithm', 'the', 'k', 'ell', 'pebble', 'game', 'with', 'colors', 'and', 'use', 'it', 'obtain', 'a', 'characterization', 'of', 'the', 'family', 'of', 'k', 'ell', 'sparse', 'graphs', 'and', 'algorithmic', 'solutions', 'to', 'a', 'family', 'of', 'problems', 'concerning', 'tree', 'decompositions', 'of', 'graphs', '.', 'Special', 'instances', 'of', 'sparse', 'graphs', 'appear', 'in', 'rigidity', 'theory', 'and', 'have', 'received', 'increased', 'attention', 'in', 'recent', 'years', '.', 'In', 'particular', 'our', 'colored', 'pebbles', 'generalize', 'and', 'strengthen', 'the', 'previous', 'results', 'of', 'Lee', 'and', 'Streinu', 'and', 'give', 'a', 'new', 'proof', 'of', 'the', 'Tutte', 'Nash', 'Williams', 'characterization', 'of', 'arboricity', '.', 'We', 'also', 'present', 'a', 'new', 'decomposition', 'that', 'certifies', 'sparsity', 'based', 'on', 'the', 'k', 'ell', 'pebble', 'game', 'with', 'colors', '.', 'Our', 'work', 'also', 'exposes', 'connections', 'between', 'pebble', 'game', 'algorithms', 'and', 'previous', 'sparse', 'graph', 'algorithms', 'by', 'Gabow', 'Gabow', 'and', 'Westermann', 'and', 'Hendrickson', '.', '</s>']\n",
      "[1, 112, 113, 90, 114, 115, 27, 116, 117, 118, 119, 70, 120, 13, 121, 122, 123, 90, 124, 5, 27, 125, 5, 116, 117, 126, 127, 13, 128, 129, 36, 90, 125, 5, 130, 131, 132, 133, 5, 127, 33, 134, 135, 5, 126, 127, 136, 20, 137, 138, 13, 139, 140, 141, 142, 20, 143, 144, 33, 145, 146, 147, 148, 149, 150, 13, 151, 27, 152, 153, 5, 154, 13, 155, 13, 156, 90, 114, 157, 5, 27, 158, 159, 160, 124, 5, 161, 33, 112, 162, 163, 90, 114, 164, 98, 165, 166, 167, 168, 27, 116, 117, 118, 119, 70, 120, 33, 169, 170, 162, 171, 172, 173, 118, 119, 174, 13, 152, 126, 175, 174, 176, 177, 177, 13, 178, 13, 179, 33, 2]\n",
      "We describe a new algorithm the k ell pebble game with colors and use it obtain a characterization of the family of k ell sparse graphs and algorithmic solutions to a family of problems concerning tree decompositions of graphs . Special instances of sparse graphs appear in rigidity theory and have received increased attention in recent years . In particular our colored pebbles generalize and strengthen the previous results of Lee and Streinu and give a new proof of the Tutte Nash Williams characterization of arboricity . We also present a new decomposition that certifies sparsity based on the k ell pebble game with colors . Our work also exposes connections between pebble game algorithms and previous sparse graph algorithms by Gabow Gabow and Westermann and Hendrickson .\n",
      "\n",
      "Word = the\n",
      "Word ID = 27\n",
      "Word decoded from ID = the\n"
     ]
    }
   ],
   "source": [
    "for ids, title, abstract, categories in data[:2]:\n",
    "    sentence = abstract\n",
    "    word_tokens = vocab.tokenized_sentence(sentence)\n",
    "\n",
    "    # Automatically adds bos_id and eos_id before and after sentence ids respectively\n",
    "    word_ids = vocab.get_ids_from_sentence(sentence)\n",
    "    print(sentence)\n",
    "    print(word_tokens)\n",
    "    print(word_ids)\n",
    "    print(vocab.decode_sentence_from_ids(word_ids))\n",
    "    print()\n",
    "\n",
    "word = \"the\"\n",
    "word_id = vocab.word_to_id[word]\n",
    "print(f\"Word = {word}\")\n",
    "print(f\"Word ID = {word_id}\")\n",
    "print(f\"Word decoded from ID = {vocab.decode_sentence_from_ids([word_id])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a4ed3cfc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-05T14:05:55.031117Z",
     "start_time": "2024-04-05T14:05:55.021549Z"
    }
   },
   "outputs": [],
   "source": [
    "class ArXiv_dataset(Dataset):\n",
    "    \"\"\"ArXiv dataset consisting of Abstract, title pairs.\"\"\"\n",
    "\n",
    "    def __init__(self, abstracts, titles, vocab, device):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            conversations: list of tuple (src_string, tgt_string)\n",
    "                         - src_string: String of the source sentence\n",
    "                         - tgt_string: String of the target sentence\n",
    "            vocab: Vocabulary object that contains the mapping of\n",
    "                    words to indices\n",
    "            device: cpu or cuda\n",
    "        \"\"\"\n",
    "        self.abstract_title = list(zip(abstracts, titles))\n",
    "        self.vocab = vocab\n",
    "        self.device = device\n",
    "\n",
    "        def encode(src, tgt):\n",
    "            src_ids = self.vocab.get_ids_from_sentence(src)\n",
    "            tgt_ids = self.vocab.get_ids_from_sentence(tgt)\n",
    "            return (src_ids, tgt_ids)\n",
    "\n",
    "        # We will pre-tokenize the conversations and save in id lists for later use\n",
    "        self.tokenized_abstract_title = [encode(src, tgt) for src, tgt in self.abstract_title]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.abstract_title)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        return {\"conv_ids\":self.tokenized_abstract_title[idx], \"conv\":self.abstract_title[idx]}\n",
    "\n",
    "def collate_fn(data):\n",
    "    \"\"\"Creates mini-batch tensors from the list of tuples (src_seq, trg_seq).\n",
    "    We should build a custom collate_fn rather than using default collate_fn,\n",
    "    because merging sequences (including padding) is not supported in default.\n",
    "    Seqeuences are padded to the maximum length of mini-batch sequences (dynamic padding).\n",
    "    Args:\n",
    "        data: list of dicts {\"conv_ids\":(src_ids, tgt_ids), \"conv\":(src_str, trg_str)}.\n",
    "            - src_ids: list of src piece ids; variable length.\n",
    "            - tgt_ids: list of tgt piece ids; variable length.\n",
    "            - src_str: String of src\n",
    "            - tgt_str: String of tgt\n",
    "    Returns: dict { \"conv_ids\":     (src_ids, tgt_ids),\n",
    "                    \"conv\":         (src_str, tgt_str),\n",
    "                    \"conv_tensors\": (src_seqs, tgt_seqs)}\n",
    "            src_seqs: torch tensor of shape (src_padded_length, batch_size).\n",
    "            trg_seqs: torch tensor of shape (tgt_padded_length, batch_size).\n",
    "            src_padded_length = length of the longest src sequence from src_ids\n",
    "            tgt_padded_length = length of the longest tgt sequence from tgt_ids\n",
    "\n",
    "    \"\"\"\n",
    "    # Sort conv_ids based on decreasing order of the src_lengths.\n",
    "    # This is required for efficient GPU computations.\n",
    "    src_ids = [torch.LongTensor(e[\"conv_ids\"][0]) for e in data]\n",
    "    tgt_ids = [torch.LongTensor(e[\"conv_ids\"][1]) for e in data]\n",
    "    src_str = [e[\"conv\"][0] for e in data]\n",
    "    tgt_str = [e[\"conv\"][1] for e in data]\n",
    "    data = list(zip(src_ids, tgt_ids, src_str, tgt_str))\n",
    "    data.sort(key=lambda x: len(x[0]), reverse=True)\n",
    "    src_ids, tgt_ids, src_str, tgt_str = zip(*data)\n",
    "\n",
    "    ### BEGIN YOUR CODE ###\n",
    "\n",
    "    # Pad the src_ids and tgt_ids using token pad_id to create src_seqs and tgt_seqs\n",
    "    src_seqs = pad_sequence(src_ids, batch_first=False, padding_value=pad_id)\n",
    "    tgt_seqs = pad_sequence(tgt_ids, batch_first=False, padding_value=pad_id)\n",
    "    ### END YOUR CODE ###\n",
    "\n",
    "    return {\"conv_ids\":(src_ids, tgt_ids), \"conv\":(src_str, tgt_str), \"conv_tensors\":(src_seqs.to(device), tgt_seqs.to(device))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e22e2734",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-05T14:06:11.177003Z",
     "start_time": "2024-04-05T14:06:09.891108Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create the DataLoader for all_conversations\n",
    "dataset = ArXiv_dataset(abstracts_only, titles_only, vocab, device)\n",
    "\n",
    "batch_size = 2\n",
    "\n",
    "data_loader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9cebb0f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-05T14:06:11.200114Z",
     "start_time": "2024-04-05T14:06:11.179037Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing first training batch of size 2\n",
      "List of source strings:\n",
      "  We investigate analytic solutions to Witten's bosonic string field theory and\n",
      "Berkovits' WZW-type superstring field theory. We construct solutions with\n",
      "parameters out of simpler ones, using a commutative monoid that includes the\n",
      "family of wedge states. Our solutions are generalizations of solutions for\n",
      "marginal deformations by nonsingular currents, and can also reproduce Schnabl's\n",
      "tachyon vacuum solution in bosonic string field theory. This implies that such\n",
      "known solutions are generated from simple solutions which are based on the\n",
      "identity state. We also discuss gauge transformations and induced field\n",
      "redefinitions for our solutions in both bosonic and super string field theory.\n",
      "\n",
      "  We study the AdS_5 x S^5 sigma-model truncated to the near-flat-space limit\n",
      "to two-loops in perturbation theory. In addition to extending previously known\n",
      "one-loop results to the full SU(2|2)^2 S-matrix we calculate the two-loop\n",
      "correction to the dispersion relation and then compute the complete two-loop\n",
      "S-matrix. The result of the perturbative calculation can be compared with the\n",
      "appropriate limit of the conjectured S-matrix for the full theory and complete\n",
      "agreement is found.\n",
      "\n",
      "\n",
      "Tokenized source ids:\n",
      "tensor([    1,   112,  1353,   453,   129,    36, 11893,   218,   309,   745,\n",
      "          186,   138,    13, 11483, 11441,  2561,   754,   186,   138,    33,\n",
      "          112,   631,   129,    70,  1558,  1059,     5,  3083,   633,   274,\n",
      "           90,   993, 14845,    98,  2713,    27,   125,     5,  9229,   774,\n",
      "           33,   169,   129,    46,  2458,     5,   129,    26,  4101,  2781,\n",
      "          176, 11443,  2560,    13,   102,   162,  3067, 23660,   218, 13617,\n",
      "         2475,   427,    20,   309,   745,   186,   138,    33,   233,  1563,\n",
      "           98,   459,   373,   129,    46,  1027,    40,   424,   129,    64,\n",
      "           46,   167,   168,    27,  4990,    54,    33,   112,   162,   506,\n",
      "         2384,  4030,    13,   460,   186, 32511,    26,   147,   129,    20,\n",
      "          352,   309,    13,  6817,   745,   186,   138,    33,     2])\n",
      "tensor([   1,  112,  293,   27, 3918,  231,  685, 3551,  188, 1028,   36,   27,\n",
      "         844, 2912,   62,  219,   36,  294, 3638,   20, 4204,  138,   33,  145,\n",
      "        2526,   36,  857, 3875,  373,  337,  389,  153,   36,   27, 3606, 3356,\n",
      "         685,  789,  269,  443,   27,  294,  389,  311,   36,   27, 5887,  380,\n",
      "          13,  525,  271,   27, 2349,  294,  389,  685,  789,   33,   59,  220,\n",
      "           5,   27,   21,   19,  102,  103,  931,   70,   27,  404,  219,    5,\n",
      "          27, 7742,  685,  789,   26,   27, 3606,  138,   13, 2349,   68,   24,\n",
      "         346,   33,    2])\n",
      "\n",
      "Padded source ids as tensor (shape torch.Size([109, 2])):\n",
      "tensor([[    1,     1],\n",
      "        [  112,   112],\n",
      "        [ 1353,   293],\n",
      "        [  453,    27],\n",
      "        [  129,  3918],\n",
      "        [   36,   231],\n",
      "        [11893,   685],\n",
      "        [  218,  3551],\n",
      "        [  309,   188],\n",
      "        [  745,  1028],\n",
      "        [  186,    36],\n",
      "        [  138,    27],\n",
      "        [   13,   844],\n",
      "        [11483,  2912],\n",
      "        [11441,    62],\n",
      "        [ 2561,   219],\n",
      "        [  754,    36],\n",
      "        [  186,   294],\n",
      "        [  138,  3638],\n",
      "        [   33,    20],\n",
      "        [  112,  4204],\n",
      "        [  631,   138],\n",
      "        [  129,    33],\n",
      "        [   70,   145],\n",
      "        [ 1558,  2526],\n",
      "        [ 1059,    36],\n",
      "        [    5,   857],\n",
      "        [ 3083,  3875],\n",
      "        [  633,   373],\n",
      "        [  274,   337],\n",
      "        [   90,   389],\n",
      "        [  993,   153],\n",
      "        [14845,    36],\n",
      "        [   98,    27],\n",
      "        [ 2713,  3606],\n",
      "        [   27,  3356],\n",
      "        [  125,   685],\n",
      "        [    5,   789],\n",
      "        [ 9229,   269],\n",
      "        [  774,   443],\n",
      "        [   33,    27],\n",
      "        [  169,   294],\n",
      "        [  129,   389],\n",
      "        [   46,   311],\n",
      "        [ 2458,    36],\n",
      "        [    5,    27],\n",
      "        [  129,  5887],\n",
      "        [   26,   380],\n",
      "        [ 4101,    13],\n",
      "        [ 2781,   525],\n",
      "        [  176,   271],\n",
      "        [11443,    27],\n",
      "        [ 2560,  2349],\n",
      "        [   13,   294],\n",
      "        [  102,   389],\n",
      "        [  162,   685],\n",
      "        [ 3067,   789],\n",
      "        [23660,    33],\n",
      "        [  218,    59],\n",
      "        [13617,   220],\n",
      "        [ 2475,     5],\n",
      "        [  427,    27],\n",
      "        [   20,    21],\n",
      "        [  309,    19],\n",
      "        [  745,   102],\n",
      "        [  186,   103],\n",
      "        [  138,   931],\n",
      "        [   33,    70],\n",
      "        [  233,    27],\n",
      "        [ 1563,   404],\n",
      "        [   98,   219],\n",
      "        [  459,     5],\n",
      "        [  373,    27],\n",
      "        [  129,  7742],\n",
      "        [   46,   685],\n",
      "        [ 1027,   789],\n",
      "        [   40,    26],\n",
      "        [  424,    27],\n",
      "        [  129,  3606],\n",
      "        [   64,   138],\n",
      "        [   46,    13],\n",
      "        [  167,  2349],\n",
      "        [  168,    68],\n",
      "        [   27,    24],\n",
      "        [ 4990,   346],\n",
      "        [   54,    33],\n",
      "        [   33,     2],\n",
      "        [  112,     0],\n",
      "        [  162,     0],\n",
      "        [  506,     0],\n",
      "        [ 2384,     0],\n",
      "        [ 4030,     0],\n",
      "        [   13,     0],\n",
      "        [  460,     0],\n",
      "        [  186,     0],\n",
      "        [32511,     0],\n",
      "        [   26,     0],\n",
      "        [  147,     0],\n",
      "        [  129,     0],\n",
      "        [   20,     0],\n",
      "        [  352,     0],\n",
      "        [  309,     0],\n",
      "        [   13,     0],\n",
      "        [ 6817,     0],\n",
      "        [  745,     0],\n",
      "        [  186,     0],\n",
      "        [  138,     0],\n",
      "        [   33,     0],\n",
      "        [    2,     0]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Test one batch of training data\n",
    "first_batch = next(iter(data_loader))\n",
    "print(f\"Testing first training batch of size {len(first_batch['conv'][0])}\")\n",
    "print(f\"List of source strings:\")\n",
    "print_list(first_batch[\"conv\"][0])\n",
    "print(f\"Tokenized source ids:\")\n",
    "print_list(first_batch[\"conv_ids\"][0])\n",
    "print(f\"Padded source ids as tensor (shape {first_batch['conv_tensors'][0].size()}):\")\n",
    "print(first_batch[\"conv_tensors\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4e77015b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-05T14:06:42.834102Z",
     "start_time": "2024-04-05T14:06:42.824026Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing first training batch of size 2\n",
      "List of target strings:\n",
      "Comments on Solutions for Nonsingular Currents in Open String Field\n",
      "  Theories\n",
      "World-sheet scattering in AdS_5 x S^5 at two loops\n",
      "\n",
      "Tokenized target ids:\n",
      "tensor([    1,  5395,   168,  6309,    26, 31260,  8651,    20, 12560,  3363,\n",
      "          194,  7554,     2])\n",
      "tensor([   1, 2607, 2904,  344,   20, 3918,  231,  685,   11,  294, 3638,    2])\n",
      "\n",
      "Padded target ids as tensor (shape torch.Size([13, 2])):\n",
      "tensor([[    1,     1],\n",
      "        [ 5395,  2607],\n",
      "        [  168,  2904],\n",
      "        [ 6309,   344],\n",
      "        [   26,    20],\n",
      "        [31260,  3918],\n",
      "        [ 8651,   231],\n",
      "        [   20,   685],\n",
      "        [12560,    11],\n",
      "        [ 3363,   294],\n",
      "        [  194,  3638],\n",
      "        [ 7554,     2],\n",
      "        [    2,     0]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(f\"Testing first training batch of size {len(first_batch['conv'][1])}\")\n",
    "print(f\"List of target strings:\")\n",
    "print_list(first_batch[\"conv\"][1])\n",
    "print(f\"Tokenized target ids:\")\n",
    "print_list(first_batch[\"conv_ids\"][1])\n",
    "print(f\"Padded target ids as tensor (shape {first_batch['conv_tensors'][1].size()}):\")\n",
    "print(first_batch[\"conv_tensors\"][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f641b1b8",
   "metadata": {},
   "source": [
    "# Model Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e121f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "667e50d9",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8bbf23e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-05T14:08:04.021654Z",
     "start_time": "2024-04-05T14:08:04.015179Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, data_loader, num_epochs, model_file, learning_rate=0.0001):\n",
    "    \"\"\"\n",
    "    Train the model for given number of epochs and save the trained model in\n",
    "    the final model_file.\n",
    "    \"\"\"\n",
    "    decoder_learning_ratio = 5.0\n",
    "\n",
    "    ### BEGIN YOUR CODE ###\n",
    "\n",
    "    encoder_parameter_names = ['embed', 'encoder', 'trans'] # <- Add a list of encoder parameter names here!\n",
    "\n",
    "    ### END YOUR CODE ###\n",
    "\n",
    "    encoder_named_params = list(filter(lambda kv: any(key in kv[0] for key in encoder_parameter_names), model.named_parameters()))\n",
    "    decoder_named_params = list(filter(lambda kv: not any(key in kv[0] for key in encoder_parameter_names), model.named_parameters()))\n",
    "    encoder_params = [e[1] for e in encoder_named_params]\n",
    "    decoder_params = [e[1] for e in decoder_named_params]\n",
    "    optimizer = torch.optim.AdamW([\n",
    "        {'params': encoder_params},\n",
    "        {\n",
    "            'params': decoder_params,\n",
    "            'lr': learning_rate * decoder_learning_ratio\n",
    "        }\n",
    "    ], lr = learning_rate)\n",
    "\n",
    "    clip = 50.0\n",
    "    for epoch in tqdm.trange(num_epochs, desc=\"training\", unit=\"epoch\"):\n",
    "        with tqdm.tqdm(data_loader, desc=f\"epoch {epoch + 1}\", unit=\"batch\", total=len(data_loader), position=0, leave=True) as batch_iterator:\n",
    "            model.train()\n",
    "            total_loss = 0.0\n",
    "            for i, batch_data in enumerate(batch_iterator, start=1):\n",
    "                source, target = batch_data[\"conv_tensors\"]\n",
    "                optimizer.zero_grad()\n",
    "                loss = model.compute_loss(source, target)\n",
    "                total_loss += loss.item()\n",
    "                loss.backward()\n",
    "\n",
    "                # Gradient clipping before taking the step\n",
    "                _ = nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "                optimizer.step()\n",
    "\n",
    "                batch_iterator.set_postfix(mean_loss=total_loss / i, current_loss=loss.item())\n",
    "\n",
    "    # Save the model after training\n",
    "    torch.save(model.state_dict(), model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ce2888",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50\n",
    "batch_size = 512\n",
    "\n",
    "# Reloading the data_loader to increase batch_size\n",
    "data_loader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "baseline_model = Seq2seqBaseline(vocab).to(device)\n",
    "train(baseline_model, data_loader, num_epochs, \"baseline_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f2a15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the model from the model file. Useful when you have already trained and saved the model\n",
    "baseline_model = Seq2seqBaseline(vocab).to(device)\n",
    "baseline_model.load_state_dict(torch.load(\"baseline_model.pt\", map_location=device))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
